{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2021,
     "status": "ok",
     "timestamp": 1681259916213,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "YfWmbQWgpRDE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1673,
     "status": "ok",
     "timestamp": 1681259918320,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "Re8aw0Muoc39"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('{파일경로}/lifelog_data_2018.csv')\n",
    "df2 = pd.read_csv('{파일경로}/lifelog_data_2019.csv')\n",
    "df3 = pd.read_csv('{파일경로}/lifelog_data_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1681259920675,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "FIGuqJvIoZ7L"
   },
   "outputs": [],
   "source": [
    "def engineering(n, d):\n",
    "    # 성별 인코딩\n",
    "    d['gender'] = d['gender'].apply(lambda x:0 if x == 'M' else 1)\n",
    "    # 날짜 변환\n",
    "    d['date'] = pd.to_datetime(d['date'])\n",
    "\n",
    "    # 스트레스(target)\n",
    "    d['pmStress'] = d['pmStress'] - 1\n",
    "    # XGBoost의 pred 값이랑 맞추기 위해 target 값에 -1\n",
    "    \n",
    "    # 감정변화비율 = 오후감정/오전감정\n",
    "    d['emotionChangeRate'] = d['pmEmotion'] / d['amEmotion']\n",
    "    # 긍정변화평균 = emotion1~7의 평균\n",
    "    d['positiveMean'] = d.filter(regex='Positive').mean(axis=1)\n",
    "    # 긴장변화평균 = emotion1~7의 평균\n",
    "    d['tensionnMean'] = d.filter(regex='Tension').mean(axis=1)\n",
    "    \n",
    "    # 긍정비율 = Positive5~7 / 1~3 > 4는 중앙값으로 진행하지 않음\n",
    "    d['positiveRate'] = d[['emotionPositive5', 'emotionPositive6', 'emotionPositive7']].sum(axis=1) / d[['emotionPositive1', 'emotionPositive2', 'emotionPositive3']].sum(axis=1)\n",
    "    # 긴장비율 = Tension5~7 / 1~3 > 4는 중앙값으로 진행하지 않음\n",
    "    d['tensionRate'] = d[['emotionTension5', 'emotionTension6', 'emotionTension7']].sum(axis=1) / d[['emotionTension1', 'emotionTension2', 'emotionTension3']].sum(axis=1)\n",
    "    \n",
    "    # 긍정감정의 임시 테이블\n",
    "    pos_temp = d.filter(regex='Positive')\n",
    "    # 가장 높은 긍정감정의 숫자(1~7)\n",
    "    d['topPositive'] = pos_temp.idxmax(axis=1).apply(lambda x:int(x[-1]))\n",
    "    # 가장 낮은 긍정감정의 숫자(1~7)\n",
    "    d['botPositive'] = pos_temp.idxmin(axis=1).apply(lambda x:int(x[-1]))\n",
    "    \n",
    "    # 긍정 수치 1~7 * count 수\n",
    "    for p, c in enumerate(pos_temp.columns):\n",
    "        d.loc[:, [c]] = d.loc[:, [c]] * (p+1)\n",
    "        \n",
    "    del pos_temp\n",
    "    \n",
    "    # 긴장감정의 임시 테이블\n",
    "    ten_temp = d.filter(regex='Tension')\n",
    "    # 가장 높은 긴장감정의 숫자(1~7)\n",
    "    d['topTension'] = ten_temp.idxmax(axis=1).apply(lambda x:int(x[-1]))\n",
    "    # 가장 낮은 긴장감정의 숫자(1~7)\n",
    "    d['botTension'] = ten_temp.idxmin(axis=1).apply(lambda x:int(x[-1]))\n",
    "    \n",
    "    # 긴장 수치 1~7 * count 수\n",
    "    for t, c in enumerate(ten_temp.columns):\n",
    "        d.loc[:, [c]] = d.loc[:, [c]] * (t+1)\n",
    "    \n",
    "    del ten_temp\n",
    "    \n",
    "    # 수치 반영 긍정 평균\n",
    "    d['positiveWMean'] = d.filter(regex='Positive').mean(axis=1)\n",
    "    # 수치 반영 긴장 평균\n",
    "    d['tensionWMean'] = d.filter(regex='Tension').mean(axis=1)\n",
    "    \n",
    "    # 수치 반영 긍정 비율\n",
    "    d['positiveWRate'] = d[['emotionPositive5', 'emotionPositive6', 'emotionPositive7']].sum(axis=1) / d[['emotionPositive1', 'emotionPositive2', 'emotionPositive3']].sum(axis=1)\n",
    "    # 수치 반영 긴장 비율\n",
    "    d['tensionWRate'] = d[['emotionTension5', 'emotionTension6', 'emotionTension7']].sum(axis=1) / d[['emotionTension1', 'emotionTension2', 'emotionTension3']].sum(axis=1)\n",
    "    \n",
    "    # 최고 수치를 반영한 오전 감정 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['aCtPT'] = round(d['amEmotion'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    # 최고 수치를 반영한 오전 컨디션 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['aEtPT'] = round(d['amCondition'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    # 최고 수치를 반영한 오후 감정 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['pEtPT'] = round(d['pmEmotion'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    \n",
    "    # (최고 수치를 반영한 오전 컨디션 + 최고 수치를 반영한 오전 감정 + 최고 수치를 반영한 오후 감정) / 3\n",
    "    # (최고 수치를 반영한 오전 컨디션 + 최고 수치를 반영한 오전 감정 + 최고 수치를 반영한 오후 감정)\n",
    "    d['aCEpEtPTm'] = round((d['aCtPT'] + d['aEtPT'] + d['pEtPT']) / 3)\n",
    "    d['aCEpEtPTm3'] = (d['aCtPT'] + d['aEtPT'] + d['pEtPT'])\n",
    "    \n",
    "    # 긍정적인지 = 수치 반영 긍정 평균이 중앙값보다 크면 1 아니면 0\n",
    "    d['positive'] = d['positiveWRate'].apply(lambda x:1 if x > d['positiveWRate'].median() else 0)\n",
    "    # 부정적인지 = 수치 반영 긍정 평균이 중앙값보다 작으면 1 아니면 0\n",
    "    d['negative'] = d['positiveWRate'].apply(lambda x:1 if x < d['positiveWRate'].median() else 0)\n",
    "    \n",
    "    # 긴장 상태인지 = 수치 반영 긴장 평균이 중앙값보다 크면 1 아니면 0\n",
    "    d['aroused'] = d['tensionWRate'].apply(lambda x:1 if x > d['tensionWRate'].median() else 0)\n",
    "    # 편안한 상태인지 = 수치 반영 긴장 평균이 중앙값보다 작으면 1 아니면 0\n",
    "    d['relaxed'] = d['tensionWRate'].apply(lambda x:1 if x < d['tensionWRate'].median() else 0)\n",
    "    \n",
    "    # 활동 비율 = 자전거, 도보의 합 / 운송수단, 가만히 있기의 합\n",
    "    d['activityRate'] = d[['on_bicycle', 'on_foot']].sum(axis=1) / d[['in_vehicle', 'still']].sum(axis=1)\n",
    "    # 0으로 나누기된 것을 0으로 치환 > inf 값을 가짐\n",
    "    d.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    d.dropna(inplace=True)\n",
    "    d.sample(n=d.shape[0], random_state=1234)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, d in enumerate([df1, df2, df3]):\n",
    "    globals() [f'df{n+1}'] = engineering(n, d.copy()) # 위에서 생성한 engineering 함수 적용\n",
    "df = pd.concat([df1, df2, df3], axis=0) # 데이터 결합 \n",
    "df = df[[c for c in df.columns if c not in ['pmStress']] + ['pmStress']] # pmStress 마지막 열로 이동\n",
    "df.drop(['date', 'userId'], axis = 1, inplace = True) # dtae, userId 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eiL1BYnz_Hmc"
   },
   "outputs": [],
   "source": [
    "# 사용한 파라미터 조합\n",
    "n_estimators = [10, 50, 100]\n",
    "max_depth = [None, 5, 10, 15, 20, 50, 100, 200, 500]\n",
    "max_features = [4, 10, 15, 20]\n",
    "min_samples_split = [2, 5, 10, 20]\n",
    "min_samples_leaf = [2, 5, 10, 20]\n",
    "criterion = ['entropy', 'gini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5227848101265822\n",
      "Iteration: 1\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.6761904761904762\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.520253164556962\n",
      "Iteration: 2\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5619047619047619\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5063291139240507\n",
      "Iteration: 3\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5714285714285714\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5088607594936709\n",
      "Iteration: 4\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5523809523809524\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5177215189873418\n",
      "Iteration: 5\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5523809523809524\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5113924050632911\n",
      "Iteration: 6\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5904761904761905\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5316455696202531\n",
      "Iteration: 7\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.6095238095238096\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.49746835443037973\n",
      "Iteration: 8\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5523809523809524\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5139240506329114\n",
      "Iteration: 9\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5619047619047619\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5139240506329115\n",
      "Iteration: 10\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5904761904761905\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5278481012658228\n",
      "Iteration: 11\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.580952380952381\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5379746835443038\n",
      "Iteration: 12\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5714285714285714\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.530379746835443\n",
      "Iteration: 13\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5714285714285714\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5227848101265822\n",
      "Iteration: 14\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5714285714285714\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5139240506329115\n",
      "Iteration: 15\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5523809523809524\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5316455696202531\n",
      "Iteration: 16\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.6\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.518987341772152\n",
      "Iteration: 17\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.580952380952381\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5240506329113923\n",
      "Iteration: 18\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5714285714285714\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5379746835443038\n",
      "Iteration: 19\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.6190476190476191\n",
      "========================================================================================================================================================================================================\n",
      "n_estimators: 100, max_depth: None, max_features: 30, min_samples_split: 4, min_samples_leaf: 4, criterion: entropy  -> accuracy: 0.5227848101265823\n",
      "Iteration: 20\n",
      "Best params: {'n_estimators': 100, 'max_depth': None, 'max_features': 30, 'min_samples_split': 4, 'min_samples_leaf': 4, 'criterion': 'entropy'}\n",
      "Accuracy on test set: 0.5619047619047619\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "random_state = 8888 # 랜덤 시드 설정\n",
    "n_est = [100] # 결정 트리 개수 설정\n",
    "max_d = [None] # 결정 트리의 최대 깊이\n",
    "max_f = [30] # 최대 특성 개수\n",
    "min_s_s = [4] # 최소 분리 샘플 수 \n",
    "min_s_l = [4] # 최소 잎새 샘플 수\n",
    "c = ['entropy'] # 결정 트리 분리 기준 \n",
    "test_accuracy_list = [] # 각 반복에서 테스트 셋의 정확도를 저장할 리스트\n",
    "results = [] # 모든 반복에서 테스트 셋 정화도를 저장할 리스트\n",
    "\n",
    "smote = SMOTE() # 오버 샘플링 기법 중 SMOTE 사용\n",
    "\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "# X : pmStress 컬럼을 제외한 데이터 \n",
    "# y : 'pmStress'\n",
    "\n",
    "for i in range(20):\n",
    "    train = pd.DataFrame() # train data를 저장할 리스트\n",
    "    valid = pd.DataFrame() # validation data를 저장할 리스트\n",
    "    test = pd.DataFrame() # test data를 저장할 리스트\n",
    "    for j in range(5):\n",
    "        train = pd.concat([train, df.loc[df['pmStress'] == np.unique(df['pmStress'])[j]][:round(len(df) / 5 * 0.9)]], axis=0) # 90% test data\n",
    "        test = pd.concat([test, df.loc[df['pmStress'] == np.unique(df['pmStress'])[j]][-round(len(df) / 5 * 0.1):]], axis=0) # 10% test data\n",
    "\n",
    "    train = train.sample(n=train.shape[0]) # , random_state=1234)\n",
    "    test = test.sample(n=test.shape[0], random_state=1234) # , random_state=1234)\n",
    "    \n",
    "    X_train, y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "    X_test, y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "    \n",
    "    # 교차 검증 정확도를 달성하는 하이퍼파라미터 조합 선택\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    for n_estimators in n_est:\n",
    "        for max_depth in max_d:\n",
    "            for max_features in max_f:\n",
    "                for min_samples_split in min_s_s:\n",
    "                    for min_samples_leaf in min_s_l:\n",
    "                        for criterion in c:\n",
    "                            score_sum = 0\n",
    "                            # 데이터를 5개의 fold로 나누고, Shuffle = True로 지정하여 데이터를 랜덤으로 섞음\n",
    "                            # k-fold 교차검증을 수행하기 위해 train set과 validation data로 분리\n",
    "                            for train_index, val_index in KFold(n_splits=5, shuffle=True, random_state=random_state).split(train):\n",
    "                                X_train_kf, X_val_kf = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                                y_train_kf, y_val_kf = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "                                # validation data가 아닌, train data에만 smote 기법을 사용\n",
    "                                X_train_kf_resampled, y_train_kf_resampled = smote.fit_resample(X_train_kf, y_train_kf)\n",
    "                                \n",
    "                                clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n",
    "                                                             min_samples_split=min_samples_split, criterion=criterion, n_jobs=-1)\n",
    "                                clf.fit(X_train_kf_resampled, y_train_kf_resampled)\n",
    "                                score_sum += accuracy_score(y_val_kf, clf.predict(X_val_kf)) #5번 교차검증의 정확도의 합\n",
    "                            score_avg = score_sum / 5 # 5번 교차검증 정확도의 평균\n",
    "                            if score_avg > best_score: # best_score보다 score_sum이 클 경우 best_score를 score_avg로 업데이트하고 헤딩 파라미터 조합을 best_params에 저장\n",
    "                                best_score = score_avg # best_score와 score_avg 값이 동일할 경우, False\n",
    "                                # 각 하이퍼파라미터의 값을 출력\n",
    "                                best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features, \n",
    "                                               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'criterion': criterion}\n",
    "                            # best_score 의 하이퍼파라미터 값 출력\n",
    "                            print(f\"n_estimators: {n_estimators}, max_depth: {max_depth}, max_features: {max_features}, \"\n",
    "                                  f\"min_samples_split: {min_samples_split}, min_samples_leaf: {min_samples_leaf}, criterion: {criterion}\",\n",
    "                                  f\" -> accuracy: {score_avg}\")\n",
    "    # 하이퍼 파라미터 값을 갖고 랜덤포레스트 모델 적합                        \n",
    "    clf = RandomForestClassifier(**best_params)\n",
    "    clf.fit(X_train_kf, y_train_kf)\n",
    "    # 하이퍼 파라미터 값을 갖고, test data로 예측한 y_pred 값\n",
    "    y_pred_rf = clf.predict(X_test)\n",
    "    # 실제 test값과 예측값 사이의 정확도\n",
    "    accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    results.append(accuracy)\n",
    "\n",
    "    print(f\"Iteration: {i+1}\") # 반복 횟수\n",
    "    print(f\"Best params: {best_params}\") # 하이퍼파라미터 조합\n",
    "    print(f\"Accuracy on test set: {accuracy}\") # accuracy\n",
    "    print('=='*70)\n",
    "\n",
    "    test_accuracy_list.append(accuracy) # 반복횟수에 따른 정확도를 저장\n",
    "\n",
    "# result_rf = pd.DataFrame({'Test Accuracy': test_accuracy_list}) # 데이터프레임 형식으로 test_accuracy_list 저장                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = pd.DataFrame(y_pred_rf)\n",
    "y_pred_rf.columns = ['RandomForest']\n",
    "y_pred_rf.to_csv('RandomForest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)\n",
    "y_test.to_csv('y_test.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNIZESIKNU+Fj8408eRmIi0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

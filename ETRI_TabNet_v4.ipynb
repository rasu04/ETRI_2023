{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127540,
     "status": "ok",
     "timestamp": 1681260427203,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "AD16quJ1F3L4",
    "outputId": "cb1aace5-1005-4067-9733-a66a12fe9740"
   },
   "outputs": [],
   "source": [
    "# pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86458,
     "status": "ok",
     "timestamp": 1681260513656,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "hG3qeuTNGucJ",
    "outputId": "aeae3ac3-8151-4c30-c437-5c24110a795d"
   },
   "outputs": [],
   "source": [
    "# pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5957,
     "status": "ok",
     "timestamp": 1681260530742,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "gRTQWoCOlkD0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('{파일경로}/lifelog_data_2018.csv')\n",
    "df2 = pd.read_csv('{파일경로}/lifelog_data_2019.csv')\n",
    "df3 = pd.read_csv('{파일경로}/lifelog_data_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1681260554161,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "pIT5TkOxFqMg"
   },
   "outputs": [],
   "source": [
    "def engineering(n, d):\n",
    "    # 성별 인코딩\n",
    "    d['gender'] = d['gender'].apply(lambda x:0 if x == 'M' else 1)\n",
    "    # 날짜 변환\n",
    "    d['date'] = pd.to_datetime(d['date'])\n",
    "\n",
    "    # 스트레스(target)\n",
    "    d['pmStress'] = d['pmStress'] - 1\n",
    "    # XGBoost의 경우는 0부터 시작 > 나중에 좀 더 찾아보기\n",
    "    # Stacking을 위해 모든 모델 pmStress 범위 변경\n",
    "    \n",
    "    # 감정변화비율 = 오후감정/오전감정\n",
    "    d['emotionChangeRate'] = d['pmEmotion'] / d['amEmotion']\n",
    "    # 긍정변화평균 = emotion1~7의 평균\n",
    "    d['positiveMean'] = d.filter(regex='Positive').mean(axis=1)\n",
    "    # 긴장변화평균 = emotion1~7의 평균\n",
    "    d['tensionnMean'] = d.filter(regex='Tension').mean(axis=1)\n",
    "    \n",
    "    # 긍정비율 = Positive5~7 / 1~3 > 4는 중앙값으로 진행하지 않음\n",
    "    d['positiveRate'] = d[['emotionPositive5', 'emotionPositive6', 'emotionPositive7']].sum(axis=1) / d[['emotionPositive1', 'emotionPositive2', 'emotionPositive3']].sum(axis=1)\n",
    "    # 긴장비율 = Tension5~7 / 1~3 > 4는 중앙값으로 진행하지 않음\n",
    "    d['tensionRate'] = d[['emotionTension5', 'emotionTension6', 'emotionTension7']].sum(axis=1) / d[['emotionTension1', 'emotionTension2', 'emotionTension3']].sum(axis=1)\n",
    "    \n",
    "    # 긍정감정의 임시 테이블\n",
    "    pos_temp = d.filter(regex='Positive')\n",
    "    # 가장 높은 긍정감정의 숫자(1~7)\n",
    "    d['topPositive'] = pos_temp.idxmax(axis=1).apply(lambda x:int(x[-1]))\n",
    "    # 가장 낮은 긍정감정의 숫자(1~7)\n",
    "    d['botPositive'] = pos_temp.idxmin(axis=1).apply(lambda x:int(x[-1]))\n",
    "    \n",
    "    # 긍정 수치 1~7 * count 수\n",
    "    for p, c in enumerate(pos_temp.columns):\n",
    "        d.loc[:, [c]] = d.loc[:, [c]] * (p+1)\n",
    "        \n",
    "    del pos_temp\n",
    "    \n",
    "    # 긴장감정의 임시 테이블\n",
    "    ten_temp = d.filter(regex='Tension')\n",
    "    # 가장 높은 긴장감정의 숫자(1~7)\n",
    "    d['topTension'] = ten_temp.idxmax(axis=1).apply(lambda x:int(x[-1]))\n",
    "    # 가장 낮은 긴장감정의 숫자(1~7)\n",
    "    d['botTension'] = ten_temp.idxmin(axis=1).apply(lambda x:int(x[-1]))\n",
    "    \n",
    "    # 긴장 수치 1~7 * count 수\n",
    "    for t, c in enumerate(ten_temp.columns):\n",
    "        d.loc[:, [c]] = d.loc[:, [c]] * (t+1)\n",
    "    \n",
    "    del ten_temp\n",
    "    \n",
    "    # 수치 반영 긍정 평균\n",
    "    d['positiveWMean'] = d.filter(regex='Positive').mean(axis=1)\n",
    "    # 수치 반영 긴장 평균\n",
    "    d['tensionWMean'] = d.filter(regex='Tension').mean(axis=1)\n",
    "    \n",
    "    # 수치 반영 긍정 비율\n",
    "    d['positiveWRate'] = d[['emotionPositive5', 'emotionPositive6', 'emotionPositive7']].sum(axis=1) / d[['emotionPositive1', 'emotionPositive2', 'emotionPositive3']].sum(axis=1)\n",
    "    # 수치 반영 긴장 비율\n",
    "    d['tensionWRate'] = d[['emotionTension5', 'emotionTension6', 'emotionTension7']].sum(axis=1) / d[['emotionTension1', 'emotionTension2', 'emotionTension3']].sum(axis=1)\n",
    "    \n",
    "    # 최고 수치를 반영한 오전 감정 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['aCtPT'] = round(d['amEmotion'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    # 최고 수치를 반영한 오전 컨디션 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['aEtPT'] = round(d['amCondition'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    # 최고 수치를 반영한 오후 감정 = (오전 감정 * (최고 감정 + 최고 긴장)) / 14\n",
    "    d['pEtPT'] = round(d['pmEmotion'] * (d['topPositive'] + d['topTension']) / 14)\n",
    "    \n",
    "    d['aCEpEtPTm'] = (d['aCtPT'] + d['aEtPT'] + d['pEtPT'])\n",
    "    \n",
    "    # 긍정적인지 = 수치 반영 긍정 평균이 중앙값보다 크면 1 아니면 0\n",
    "    d['positive'] = d['positiveWRate'].apply(lambda x:1 if x > d['positiveWRate'].median() else 0)\n",
    "    # 부정적인지 = 수치 반영 긍정 평균이 중앙값보다 작으면 1 아니면 0\n",
    "    d['negative'] = d['positiveWRate'].apply(lambda x:1 if x < d['positiveWRate'].median() else 0)\n",
    "    \n",
    "    # 긴장 상태인지 = 수치 반영 긴장 평균이 중앙값보다 크면 1 아니면 0\n",
    "    d['aroused'] = d['tensionWRate'].apply(lambda x:1 if x > d['tensionWRate'].median() else 0)\n",
    "    # 편안한 상태인지 = 수치 반영 긴장 평균이 중앙값보다 작으면 1 아니면 0\n",
    "    d['relaxed'] = d['tensionWRate'].apply(lambda x:1 if x < d['tensionWRate'].median() else 0)\n",
    "    \n",
    "    # 활동 비율 = 자전거, 도보의 합 / 운송수단, 가만히 있기의 합\n",
    "    d['activityRate'] = d[['on_bicycle', 'on_foot']].sum(axis=1) / d[['in_vehicle', 'still']].sum(axis=1)\n",
    "    # 0으로 나누기된 것을 0으로 치환 > inf 값을 가짐\n",
    "    d.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    d.dropna(inplace=True)\n",
    "    d.sample(n=d.shape[0], random_state=47)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, d in enumerate([df1, df2, df3]):\n",
    "    globals() [f'df{n+1}'] = engineering(n, d.copy())\n",
    "df = pd.concat([df1, df2, df3], axis=0)\n",
    "df = df[[c for c in df.columns if c not in ['pmStress']] + ['pmStress']]\n",
    "df.drop(['date', 'userId'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1483218,
     "status": "ok",
     "timestamp": 1681262714365,
     "user": {
      "displayName": "김진솔",
      "userId": "11702173973778170953"
     },
     "user_tz": -540
    },
    "id": "Usa6NvWw6lnw",
    "outputId": "2539e6f7-8070-4572-d7fb-6a1000368c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3652173913043478\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3826086956521739\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.33043478260869563\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.4434782608695652\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.2956521739130435\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.34782608695652173\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3391304347826087\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3739130434782609\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.391304347826087\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.40869565217391307\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3739130434782609\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.34782608695652173\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.34782608695652173\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.391304347826087\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.45217391304347826\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3391304347826087\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3217391304347826\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.34782608695652173\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.3565217391304348\n",
      "============================== \n",
      " <class 'torch.optim.adam.Adam'> {'lr': 0.01} 4 24 1.3 10 0.95 entmax 16 logloss True \n",
      " accuracy : 0.4260869565217391\n"
     ]
    }
   ],
   "source": [
    "test_accuracy_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    train = pd.DataFrame()\n",
    "    valid = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for i in range(5):\n",
    "        train = pd.concat([train, df.loc[df['pmStress'] == np.unique(df['pmStress'])[i]][:round(len(df) / 5 * 0.7)]], axis=0)\n",
    "        valid = pd.concat([valid, df.loc[df['pmStress'] == np.unique(df['pmStress'])[i]][round(len(df) / 5 * 0.7):-round(len(df) / 5 * 0.1)]], axis=0)\n",
    "        test = pd.concat([test, df.loc[df['pmStress'] == np.unique(df['pmStress'])[i]][-round(len(df) / 5 * 0.1):]], axis=0)\n",
    "\n",
    "        # 동 수로 뽑을 때\n",
    "    #     temp = df.loc[df['pmStress'] == np.unique(df['pmStress'])[i]][:112]\n",
    "    #     train = pd.concat([train, temp[:round(112 * 0.7)]], axis=0)\n",
    "    #     valid = pd.concat([valid, temp[round(112 * 0.7):-round(112 * 0.1)]], axis=0)\n",
    "    #     test = pd.concat([test, temp[-round(112 * 0.1):]], axis=0)\n",
    "\n",
    "    train = train.sample(n=train.shape[0]) # , random_state=1234)\n",
    "    valid = valid.sample(n=valid.shape[0]) # , random_state=1234)\n",
    "    test = test.sample(n=test.shape[0]) # , random_state=1234)\n",
    "    \n",
    "    df = pd.concat([train, test, valid], ignore_index=True, axis=0)\n",
    "    df = df[[c for c in df.columns if c not in ['userId', 'date']]]\n",
    "\n",
    "    # train_indices = range(390) # train.index\n",
    "    # valid_indices = range(390, 505) # valid.index\n",
    "    # test_indices = range(505, 560) # test.index\n",
    "    train_indices = train.index\n",
    "    valid_indices = valid.index\n",
    "    test_indices = test.index\n",
    "\n",
    "    nunique = df.nunique()\n",
    "    types = df.dtypes\n",
    "\n",
    "    categorical_columns = []\n",
    "    categorical_dims =  {}\n",
    "    for col in df.columns:\n",
    "        if types[col] == 'object' or nunique[col] <= 7:\n",
    "            categorical_columns.append(col)\n",
    "            categorical_dims[col] = len(np.unique(df[col]))\n",
    "\n",
    "    features = [col for col in df.columns if col not in ['pmStress']] \n",
    "    cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "    cat_dims = [categorical_dims[f] for f in features if f in categorical_columns]\n",
    "\n",
    "    X_train = df[features].values[train_indices]\n",
    "    y_train = df['pmStress'].values[train_indices].reshape(-1, 1)\n",
    "\n",
    "    X_valid = df[features].values[valid_indices]\n",
    "    y_valid = df['pmStress'].values[valid_indices].reshape(-1, 1)\n",
    "\n",
    "    X_test = df[features].values[test_indices]\n",
    "    y_test = df['pmStress'].values[test_indices].reshape(-1, 1)\n",
    "\n",
    "    of = optim.Adam\n",
    "    op = dict(lr=0.01)\n",
    "    ns = 4\n",
    "    nd = 24\n",
    "    g = 1.3\n",
    "    ss = 10\n",
    "    gm = 0.95\n",
    "    mt = 'entmax'\n",
    "    bs = 16\n",
    "    em = 'logloss'\n",
    "    ws = True\n",
    "    clf = TabNetMultiTaskClassifier(optimizer_fn=of, optimizer_params=op, scheduler_fn=optim.lr_scheduler.StepLR,\n",
    "    n_steps=ns, n_d=nd, n_a=nd, gamma=g,\n",
    "    scheduler_params={\"step_size\":ss, \"gamma\":gm}, \n",
    "    mask_type=mt, verbose=0)\n",
    "\n",
    "    clf.fit(X_train.copy(), y_train.copy(),\n",
    "            eval_set=[(X_valid.copy(), y_valid.copy())],\n",
    "            loss_fn=[torch.nn.functional.cross_entropy]*5,\n",
    "            max_epochs=30, \n",
    "            patience=0,\n",
    "            batch_size=bs,\n",
    "            eval_metric=[em],\n",
    "            warm_start=ws)\n",
    "    pred_TabNet = clf.predict(X_test.copy())\n",
    "    accuracy = accuracy_score(y_test.copy(), np.array(pred_TabNet.copy()[0]).astype('float').reshape(-1, 1))\n",
    "    print(\"=\"*30, '\\n', of, op, ns, nd, g, ss, gm, mt, bs, em, ws, '\\n', 'accuracy :', accuracy)\n",
    "    test_accuracy_list.append(accuracy)\n",
    "\n",
    "# result_TabNet = pd.DataFrame({'Test Accuracy': test_accuracy_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "MTPEailqE9Hw"
   },
   "outputs": [],
   "source": [
    "y_pred_TabNet = pd.DataFrame(pred_TabNet)\n",
    "y_pred_TabNet = y_pred_TabNet.transpose()\n",
    "y_pred_TabNet.columns = ['TabNet']\n",
    "y_pred_TabNet.to_csv('TabNet.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
